{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52248863",
   "metadata": {},
   "source": [
    "# Working with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313ba00",
   "metadata": {},
   "source": [
    "One of the most common operations when working with [Amazon S3 (Amazon Simple Storage Service)](https://aws.amazon.com/s3/) is to pull data from s3 to local as well as push data from local to s3. We can use aws command line tool to achieve this:\n",
    "\n",
    "```bash\n",
    "# e.g. from s3 to local, add --recursive if it's a directory\n",
    "aws s3 cp <s3 path> <local path> --recursive\n",
    "```\n",
    "\n",
    "We'll also demonstrate how to use `boto3` to perform these kind of operations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839c141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2023-05-19\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 8.8.0\n",
      "\n",
      "pandas : 1.5.2\n",
      "pyarrow: 10.0.1\n",
      "boto3  : 1.26.49\n",
      "numpy  : 1.23.2\n",
      "json   : 2.0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from time import perf_counter\n",
    "from typing import List\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c409771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace these top level configuration, especially s3 region and bucket\n",
    "region_name = \"us-east-1\"\n",
    "s3_bucket = \"\"\n",
    "s3_json_path = \"ethenliu/test.json\"\n",
    "s3_dir = \"ethenliu/data\"\n",
    "local_dir = \"ethenliu/data\"\n",
    "s3_parquet_path = os.path.join(s3_dir, \"test.parquet\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de9f49",
   "metadata": {},
   "source": [
    "Suppose we have a python object in memory, one option is to use client's [`put_object`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object.html) method and save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd1762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [1, 2], 'embeddings': [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json dumps doesn't allow saving numpy array directly, we need to convert it to a list\n",
    "prediction = {\n",
    "    \"ids\": [1, 2],\n",
    "    \"embeddings\": np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]).tolist()\n",
    "}\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b855bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.put_object(\n",
    "    Body=json.dumps(prediction),\n",
    "    Bucket=s3_bucket,\n",
    "    Key=s3_json_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12465ca",
   "metadata": {},
   "source": [
    "### Upload and Download Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b73cdb",
   "metadata": {},
   "source": [
    "All of this is well and good until we work with some large python objects, which we'll encounter [errors](https://stackoverflow.com/questions/26319815/entitytoolarge-error-when-uploading-a-5g-file-to-amazon-s3) such as entity too large error.\n",
    "\n",
    "Directly copied from S3's [documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html)\n",
    "\n",
    "> - Upload an object in a single operation by using the AWS SDKs, REST API, or AWS CLI – With a single PUT operation, you can upload a single object up to 5 GB in size.\n",
    "> - Upload an object in parts by using the AWS SDKs, REST API, or AWS CLI – Using the multipart upload API operation, you can upload a single large object, up to 5 TB in size.\n",
    "\n",
    "Fortunately, we can rely on [`upload_file`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method, boto3 will automatically use [multipart upload underneath the hood](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html) without us having to worry about [lower level functions related to multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html). The following code chunk shows how to save our python object as a parquet file and upload it to s3 as well as downloading files from s3 to local and reading it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccce0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet_to_s3(data, s3_bucket: str, s3_path: str, verbose: bool = False):\n",
    "    \"\"\"Saves the dictionary as a parquet file and push it to s3.    \n",
    "    \"\"\"\n",
    "    file_name = os.path.split(s3_path)[-1]\n",
    "    pa_table = pa.table(data)\n",
    "    pq.write_table(pa_table, file_name)\n",
    "\n",
    "    s3_client.upload_file(Filename=file_name, Bucket=s3_bucket, Key=s3_path)\n",
    "    os.remove(file_name)\n",
    "    if verbose:\n",
    "        print(pa_table)\n",
    "        print(\"Finish writing {} to s3://{}/{}\".format(file_name, s3_bucket, s3_path))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166dec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2d numpy array to list of 1d numpy array as pyarrow supports saving 1d numpy array\n",
    "embeddings = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "prediction = {\n",
    "    \"ids\": [1, 2],\n",
    "    \"embeddings\": [embedding for embedding in embeddings]\n",
    "}\n",
    "save_as_parquet_to_s3(prediction, s3_bucket, s3_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fd1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_s3(\n",
    "    s3_bucket: str,\n",
    "    s3_path: str,\n",
    "    local_path: str,\n",
    "    n_jobs: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Download files under a s3 bucket & path to a specified local path.\n",
    "    n_jobs configures the number of threads used for downloading files in parallel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    download_paths : \n",
    "        list of files that were downloaded their corresponding local path.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    def download_file(object_key, local_path, s3_client, s3_bucket):\n",
    "        download_path = os.path.join(local_path, os.path.split(object_key)[-1])\n",
    "        s3_client.download_file(s3_bucket, object_key, download_path)\n",
    "        return download_path\n",
    "\n",
    "    # use clients instead of resource, as they are thread safe\n",
    "    # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/clients.html#multithreading-or-multiprocessing-with-clients\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # leverage paginator to avoid list object's limitation of returning 1000 objects at a time\n",
    "    # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    page_iterator = paginator.paginate(Bucket=s3_bucket, Prefix=s3_path)\n",
    "\n",
    "    parallel = Parallel(n_jobs=n_jobs, backend=\"threading\")\n",
    "    page_download_paths = []\n",
    "    for page in page_iterator:\n",
    "        object_keys = [content[\"Key\"] for content in page[\"Contents\"] if \"_SUCCESS\" not in content[\"Key\"]]\n",
    "        download_paths = parallel(\n",
    "            delayed(download_file)(object_key, local_path, s3_client, s3_bucket)\n",
    "            for object_key in object_keys\n",
    "        )\n",
    "        page_download_paths.extend(download_paths)\n",
    "\n",
    "    return page_download_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd8dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids       embeddings\n",
       "0    1  [1.0, 2.0, 3.0]\n",
       "1    2  [4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(local_dir):\n",
    "    print(\"download files from s3\")\n",
    "    start = perf_counter()\n",
    "    files = download_files_from_s3(s3_bucket, s3_dir, local_dir)\n",
    "    end = perf_counter()\n",
    "    print(\"download files from s3 elapsed: \", end - start)\n",
    "\n",
    "df = pd.read_parquet(local_dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0963406",
   "metadata": {},
   "source": [
    "Instead of downloading our parquet files to disk first, we can also read it directly into memory by wrapping the bytes object in a pyarrow `BufferReader` followed with `read_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955d5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_object(s3_bucket: str, s3_dir: str):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "    objects = bucket.objects.filter(Prefix=s3_dir)\n",
    "    objects = [obj for obj in objects if \"_SUCCESS\" not in obj.key]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb984fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_parquet_object(s3_object):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/58061225/read-a-parquet-bytes-object-in-python\n",
    "    \"\"\"\n",
    "    body = s3_object.get()[\"Body\"].read()\n",
    "    reader = pa.BufferReader(body)\n",
    "    table = pq.read_table(reader)\n",
    "    df = table.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a326b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids       embeddings\n",
       "0    1  [1.0, 2.0, 3.0]\n",
       "1    2  [4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_objects = list_s3_object(s3_bucket, s3_dir)\n",
    "df_list = [read_s3_parquet_object(s3_object) for s3_object in s3_objects]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d78a0",
   "metadata": {},
   "source": [
    "### Additional Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf338b77",
   "metadata": {},
   "source": [
    "When conducting heavy write operations, we might encounter slow down: please reduce your request rate related [errors](https://stackoverflow.com/questions/58433594/aws-s3-slowdown-please-reduce-your-request-rate). Although Amazon S3 has announced [performance improvements](https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/), it might not be enough for our use case. When countered with these type of situations, the two most common suggestions would be to: 1. add retry. 2. add prefixes/partitions.\n",
    "\n",
    "When encountering 503 slown down error or other errors that we suspect to be not from our client, we could leverage built-in retry with exponential backoff and jitter capability [[3]](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html) [[4]](https://docs.aws.amazon.com/general/latest/gr/api-retries.html). Meaning after we receive a server or throttling related error, we use progressively longer wait time with some noise added to it between each retries. For example, we can explicitly configure our boto3 client with the retry max attempt as well as retry mode (algorithm that's used for conducting the retry). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be3809",
   "metadata": {},
   "source": [
    "```python\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(\n",
    "    retries={\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\"\n",
    "    }\n",
    ")\n",
    "s3_client = boto3.client(\"s3\", config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c42fc1",
   "metadata": {},
   "source": [
    "If then new limits and adjusting retry still prove to be insufficient. Prefixes would need to be used, which is adding any string between a bucket name and an object name, for example:\n",
    "\n",
    "- bucket/1/file\n",
    "- bucket/2/file\n",
    "\n",
    "Prefixes of the object `file` would be: `/1/`, `/2/`. In this example, if we spread write across all 2 prefixes evenly, we can achieve double the throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb5a35",
   "metadata": {},
   "source": [
    "```python\n",
    "import random\n",
    "\n",
    "# we can configure the partition base on use case\n",
    "num_partition = 10\n",
    "random_part = str(random.randint(1, num_partition))\n",
    "key = os.path.join(s3_path, random_part, file_name)\n",
    "s3_client.upload_file(Filename=file_name, Bucket=s3_bucket, Key=key)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23af10",
   "metadata": {},
   "source": [
    "The offical documentation also has further suggestions on optimizing S3 performance [[2]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef361d62",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32e410",
   "metadata": {},
   "source": [
    "- [[1]](https://www.learnaws.org/2022/07/13/boto3-upload-files-s3/) Blog: How to use Boto3 to upload files to an S3 Bucket?\n",
    "- [[2]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html) AWS Documentation: Best practices design patterns: optimizing Amazon S3 performance\n",
    "- [[3]](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html) Boto3 Documentation: Retries\n",
    "- [[4]](https://docs.aws.amazon.com/general/latest/gr/api-retries.html) AWS Documentation: Error retries and exponential backoff in AWS\n",
    "- [[5]](https://www.learnaws.org/2022/10/12/boto3-download-multiple-files-s3/) Blog: How to use Boto3 to download multiple files from S3 in parallel?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
