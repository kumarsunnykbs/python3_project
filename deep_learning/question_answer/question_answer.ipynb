{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b3ea0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    html {\n",
       "        font-size: 18px !important;\n",
       "    }\n",
       "\n",
       "    body {\n",
       "        background-color: #FFF !important;\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    body .notebook-app {\n",
       "        background-color: #FFF !important;\n",
       "    }\n",
       "\n",
       "    #header {\n",
       "        box-shadow: none !important;\n",
       "    }\n",
       "\n",
       "    #notebook {\n",
       "        padding-top: 0px;\n",
       "    }\n",
       "\n",
       "    #notebook-container {\n",
       "        box-shadow: none;\n",
       "        -webkit-box-shadow: none;\n",
       "        padding: 10px;\n",
       "    }\n",
       "\n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border: 1px dashed #CCCCCC;\n",
       "    }\n",
       "\n",
       "    .edit_mode div.cell.selected {\n",
       "        border: 1px dashed #828282;\n",
       "    }\n",
       "\n",
       "    div.output_wrapper {\n",
       "        margin-top: 8px;\n",
       "    }\n",
       "\n",
       "    a {\n",
       "        color: #383838;\n",
       "    }\n",
       "\n",
       "    code,\n",
       "    kbd,\n",
       "    pre,\n",
       "    samp {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem !important;\n",
       "    }\n",
       "\n",
       "    h1 {\n",
       "        font-size: 2rem !important;\n",
       "        font-weight: 500 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: uppercase !important;\n",
       "    }\n",
       "\n",
       "    h2 {\n",
       "        font-size: 1.8rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: none !important;\n",
       "    }\n",
       "\n",
       "    h3 {\n",
       "        font-size: 1.5rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        font-style: italic !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    h4,\n",
       "    h5,\n",
       "    h6 {\n",
       "        font-size: 1rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    .prompt {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem;\n",
       "        text-align: right;\n",
       "        line-height: 1.21429rem;\n",
       "    }\n",
       "\n",
       "    /* INTRO PAGE */\n",
       "\n",
       "    .toolbar_info,\n",
       "    .list-container {\n",
       "        ;\n",
       "    }\n",
       "    /* NOTEBOOK */\n",
       "\n",
       "    div#header-container {\n",
       "        display: none !important;\n",
       "    }\n",
       "\n",
       "    div#notebook {\n",
       "        border-top: none;\n",
       "        font-size: 1rem;\n",
       "    }\n",
       "\n",
       "    div.input_prompt {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .code_cell div.input_prompt:after,\n",
       "    div.output_prompt:after {\n",
       "        content: '\\25b6';\n",
       "    }\n",
       "\n",
       "    div.output_prompt {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    div.input_area {\n",
       "        border-radius: 0px;\n",
       "        border: 1px solid #d8d8d8;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .rendered_html pre,\n",
       "    .rendered_html table,\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        border: 1px #828282 solid;\n",
       "        font-size: 0.75rem;\n",
       "        font-family: 'Menlo', monospace;\n",
       "    }\n",
       "\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        padding: 5px 10px;\n",
       "    }\n",
       "\n",
       "    .rendered_html th {\n",
       "        font-weight: normal;\n",
       "        background: #f8f8f8;\n",
       "    }\n",
       "\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "\n",
       "    div.output_html {\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    table.dataframe tr {\n",
       "        border: 1px #CCCCCC;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border-radius: 0px;\n",
       "    }\n",
       "\n",
       "    div.cell.edit_mode {\n",
       "        border-radius: 0px;\n",
       "        border: thin solid #CF5804;\n",
       "    }\n",
       "\n",
       "    span.ansiblue {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    span.ansigray {\n",
       "        color: #d8d8d8;\n",
       "    }\n",
       "\n",
       "    span.ansigreen {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    span.ansipurple {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    span.ansired {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    span.ansiyellow {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    div.output_stderr {\n",
       "        background-color: #D43132;\n",
       "    }\n",
       "\n",
       "    div.output_stderr pre {\n",
       "        color: #e8e8e8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython.CodeMirror {\n",
       "        background: #F8F8F8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #e8e8e8 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-gutters {\n",
       "        background: #F8F8F8;\n",
       "        border-right: 0px;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-linenumber {\n",
       "        color: #b8b8b8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-cursor {\n",
       "        border-left: 1px solid #585858 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-atom {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-property,\n",
       "    .cm-s-ipython span.cm-attribute {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable-2 {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-error {\n",
       "        background: #FFBDBD;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-tag {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-link {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-matchingbracket {\n",
       "        text-decoration: underline;\n",
       "         !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(css_style='custom2.css', plot_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc1ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2022-12-10\n",
      "\n",
      "torch       : 1.10.0a0+git36449ea\n",
      "datasets    : 2.7.1\n",
      "transformers: 4.21.3\n",
      "numpy       : 1.23.2\n",
      "evaluate    : 0.3.0\n",
      "pandas      : 1.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import datasets\n",
    "import collections\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from time import perf_counter\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    disable_progress_bar\n",
    ")\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    IntervalStrategy\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cache_dir = None\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db14354",
   "metadata": {},
   "source": [
    "# Fine Tuning Pre-trained Encoder on Question Answer Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f634f7d",
   "metadata": {},
   "source": [
    "In this document, we'll be going over how to train an extractive question and answer model using a pre-trained language encoder model via huggingface's transformers library. Throughout this process, we'll also:\n",
    "\n",
    "- Introduce some advanced tokenizer functionalities that are needed for token level based tasks such as question answering.\n",
    "- Explain some of the pre-processing and post-processing that are needed for question and answering task.\n",
    "\n",
    "There are many different forms of question answering, but the one we will be discussing today is termed open book extractive question answering. Open book allows our model to retrieve relevant information from some context, similar to open book exams where students can refer to their books for relevant information during an exam, in this setup, our model can look up information from external sources. Extractive means our model will extract the most relevant span of texts or snippets from these contexts to answer incoming question. Although span based answers are more constrained compared to free form answers, they come with the benefit of being easier to evaluate.\n",
    "\n",
    "Similar to a lot modern recommendation systems out there, there are three main components to these type of systems: a vector database for storing our data encoded in vector representation, a retrieval model for efficiently retrieving top-N context, lastly a reader model that identifies the span of text from a range of context. In this document, we'll be focusing on the reader model part.\n",
    "\n",
    "To piggyback on modern today's pre-trained language model for reader model fine-tuning. We need two inputs: question and context, as well as two labels identifying answer's start and end positions within that context. The following diagram depicts this notion very nicely [[4]](https://www.pinecone.io/learn/reader-models/).\n",
    "\n",
    "<img src=\"imgs/finetune_question_answer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "Slightly more formally, after feeding our input sentence through an encoder layer and obtaining the embedding vector $\\mathbf{h}^{(i)}$ for every $i_{th}$ token, we learn two additional weights, one for start position, $\\mathbf{W}_s$ and the other for end position, $\\mathbf{W}_e$. These two weights will be used to define: for each token, the probability distribution of belonging to start and end position: $\\text{softmax}(\\mathbf{h}^{(i)}\\mathbf{W}_s)$, $\\text{softmax}(\\mathbf{h}^{(i)}\\mathbf{W}_e)$\n",
    "\n",
    "The dataset we'll be using is [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Standford Question Answering Dataset). This data contains a question, a context, and potentially answer. Where the answer to every question is a segment of text, a.k.a span, from a corresponding context. We can decide whether to experiment with SQuAD or SQuAD 2.0. SQuAD version 2.0 is a superset of the existing dataset containing unanswerable questions. This nature makes it more challenging to do well on version 2.0, as not only does the model need to identify correct answers, but also need to determine when no answer is supported by a given context and abstain from spitting out unreliable guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3b0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with different public model checkpoints\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "task_name = \"squad\" # \"squad_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf7b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 377.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(task_name, cache_dir=cache_dir)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f4024",
   "metadata": {},
   "source": [
    "Printing out a sample format, hopefully field names are all quite self explanatory. The one thing that's worth clarifying is `answer_start` field contains starting character index of each answer inside the corresponding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8752e4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c4a23",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3f4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c805e3",
   "metadata": {},
   "source": [
    "After passing raw text through a tokenizer, a single word can be split into multiple tokens. e.g. in the example below, `@huggingface` is split into multiple tokens, `@` `hugging`, and `##face`. This can cause some issues for our token level labels, as our original label was mapped to a single word `@huggingface`. To resolve this, we'll need to use offsets mapping returned by the tokenizer, which gives us a tuple indicating each sub token's start and end position relative to the original token it was split from. For special tokens, offset mapping's start and end position will both be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0eca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1030, 17662, 12172, 102], 'attention_mask': [1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 8), (8, 12), (0, 0)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"@huggingface\"\n",
    "tokenized = tokenizer(word, return_offsets_mapping=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac8b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_string(tokenizer, input_ids):\n",
    "    strings = []\n",
    "    for input_id in input_ids:\n",
    "        string = tokenizer.convert_ids_to_tokens(input_id)\n",
    "        strings.append(string)\n",
    "\n",
    "    return strings\n",
    "\n",
    "\n",
    "def convert_offset_mapping_to_string(tokenized, offset_mapping, word):\n",
    "    strings = []\n",
    "    for offset in offset_mapping:\n",
    "        start = offset[0]\n",
    "        end = offset[1]\n",
    "        if end != 0:\n",
    "            strings.append(word[start:end])\n",
    "            \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids' string:  ['[CLS]', '@', 'hugging', '##face', '[SEP]']\n",
      "offset mapping string:  ['@', 'hugging', 'face']\n"
     ]
    }
   ],
   "source": [
    "# excluding for special tokens, the two should be identical\n",
    "strings = convert_id_to_string(tokenizer, tokenized[\"input_ids\"])\n",
    "print(\"input ids' string: \", strings)\n",
    "\n",
    "strings = convert_offset_mapping_to_string(tokenizer, tokenized[\"offset_mapping\"], word)\n",
    "print(\"offset mapping string: \", strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14595ce4",
   "metadata": {},
   "source": [
    "Another specific preprocessing detail for question answering task is appropriate ways to deal with long documents. In many other tasks, we typically truncate documents that are longer than our model's maximum sequence/sentence length, but here, removing some parts of the context might result in losing a section of the document that contains our answer. To deal with this, we will allow one (long) example in our dataset to give several input features by turning on `return_overflowing_tokens`. Commonly referred to as chunks, each chunk's length will be shorter than the model's maximum length (configurable hyper-parameter). Also, just in case a particular answer lies at the point where we splitted a long context, we will allow some overlap between chunks/features controlled by a hyper-parameter `doc_stride`, sometimes commonly known as sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192ff569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  2\n",
      "number of tokenized features:  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2057, 2024, 2183, 2000, 102], [101, 2183, 2000, 3975, 2023, 102], [101, 3975, 2023, 6251, 102], [101, 2023, 6251, 2003, 2936, 102], [101, 2003, 2936, 1010, 2057, 102], [101, 1010, 2057, 2024, 2036, 102], [101, 2024, 2036, 2183, 2000, 102], [101, 2183, 2000, 3975, 2009, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    \"We are going to split this sentence\",\n",
    "    \"This sentence is longer, we are also going to split it\"\n",
    "]\n",
    "tokenized = tokenizer(\n",
    "    examples,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    max_length=6,\n",
    "    stride=2\n",
    ")\n",
    "print(\"number of examples: \", len(examples))\n",
    "print(\"number of tokenized features: \", len(tokenized[\"input_ids\"]))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67828de5",
   "metadata": {},
   "source": [
    "Our two input sentences/examples has been split into 8 tokenized features. From the `overflow_to_sample_mapping` field, we can see which original example these 8 features map to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e80e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk:  [CLS] we are going to [SEP]\n",
      "Orignal input:  We are going to split this sentence\n",
      "Chunk:  [CLS] going to split this [SEP]\n",
      "Orignal input:  We are going to split this sentence\n",
      "Chunk:  [CLS] split this sentence [SEP]\n",
      "Orignal input:  We are going to split this sentence\n",
      "Chunk:  [CLS] this sentence is longer [SEP]\n",
      "Orignal input:  This sentence is longer, we are also going to split it\n",
      "Chunk:  [CLS] is longer, we [SEP]\n",
      "Orignal input:  This sentence is longer, we are also going to split it\n",
      "Chunk:  [CLS], we are also [SEP]\n",
      "Orignal input:  This sentence is longer, we are also going to split it\n",
      "Chunk:  [CLS] are also going to [SEP]\n",
      "Orignal input:  This sentence is longer, we are also going to split it\n",
      "Chunk:  [CLS] going to split it [SEP]\n",
      "Orignal input:  This sentence is longer, we are also going to split it\n"
     ]
    }
   ],
   "source": [
    "# if we print out the batched input ids, we'll see each one\n",
    "# of our sentences has been split to multiple chunks/features\n",
    "for input_id, sample_mapping in zip(tokenized[\"input_ids\"], tokenized[\"overflow_to_sample_mapping\"]):\n",
    "    chunk = tokenizer.decode(input_id)\n",
    "    print(\"Chunk: \", chunk)\n",
    "    print(\"Orignal input: \", examples[sample_mapping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed09e6d",
   "metadata": {},
   "source": [
    "Last thing we'll mention is the `sequence_ids` attribute. When feeding pairs of input to a tokenizer, we can use it to distinguish first and second portion of a given sentence. In question and answering this will be helpful for identifying whether the predicted answer's start and end position falls inside context portion of a given document, instead of question portion. If we look at a sample output, we'll notice that special tokens will be mapped to `None`, whereas our context, which is passed as the second part of our paired input will receive a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3738cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, None, 1, 1, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(\n",
    "    [\"question section\"],\n",
    "    [\"context section\"]\n",
    ")\n",
    "tokenized.sequence_ids(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b9abe",
   "metadata": {},
   "source": [
    "Upon introducing these advanced tokenizer usages, the next few code cell showcase how to put them in use and creates a function for preprocessing our question answer dataset into a format that's suited for downstream modeling. Note:\n",
    "\n",
    "- When performing truncation, we should only truncate the context, never the question. Configured via `truncation=\"only_second\"`\n",
    "- Given that we split a single document into several chunks, it can happen that a given chunk doesn't contain a valid answer, in this case, we will set question answer task's label, `start_position` and `end_position`, to index 0 (special token `[CLS]`'s index).\n",
    "- We'll be padding every feature to maximum length, as most of the context will be reaching that threshold, there's no real benefit of performing dynamic padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dc6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum length of a feature (question and context)\n",
    "max_length = 384\n",
    "# overlap between two part of the context\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818afae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_qa_train(examples):\n",
    "    \"\"\"Prepare training data, input features plus label for question answering dataset.\"\"\"\n",
    "    answers = examples[\"answers\"]\n",
    "    examples[\"question\"] = [question.strip() for question in examples[\"question\"]]\n",
    "    \n",
    "    # Tokenize our examples with truncation and padding, but keep overflows using a stride.\n",
    "    # This results in one example potentially generating several features when a context is\n",
    "    # long, each of those features having a context that overlaps a bit the previous\n",
    "    # feature's context to prevent chopping off answer span.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=doc_stride,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "     # We will label impossible answers with CLS token's index.\n",
    "    cls_index = 0\n",
    "\n",
    "    # start_positions and end_positions will be the labels for extractive question answering\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        \n",
    "        # if no answers are given, set CLS index as answer\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # find the context's corresponding start and end token index\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # if answer is within the context offset, move the token_start_index and token_end_index\n",
    "            # to two ends of the answer else label it with cls index\n",
    "            offset_start_char = offset[token_start_index][0]\n",
    "            offset_end_char = offset[token_end_index][1]\n",
    "            if offset_start_char <= start_char and offset_end_char >= end_char:\n",
    "                while token_start_index < len(offset) and offset[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_position = token_start_index - 1\n",
    "\n",
    "                while offset[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_position = token_end_index + 1\n",
    "\n",
    "                tokenized_examples[\"start_positions\"].append(start_position)\n",
    "                tokenized_examples[\"end_positions\"].append(end_position)\n",
    "            else:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15c82e",
   "metadata": {},
   "source": [
    "We test our preprocessing function on a sample text to ensure our somewhat complicated preprocessing function works as expected, i.e. the start and end position of a tokenized answer matches the original un-tokenized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17b263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected answer: Saint Bernadette Soubirous\n",
      "preprocessing answer: saint bernadette soubirous\n",
      "expected answer: a copper statue of Christ\n",
      "preprocessing answer: a copper statue of christ\n"
     ]
    }
   ],
   "source": [
    "examples = datasets[\"train\"][0:2]\n",
    "answers = examples[\"answers\"]\n",
    "\n",
    "tokenized_examples = prepare_qa_train(examples)\n",
    "\n",
    "start_positions = tokenized_examples[\"start_positions\"]\n",
    "end_positions = tokenized_examples[\"end_positions\"]\n",
    "for i, input_ids in enumerate(tokenized_examples[\"input_ids\"]):\n",
    "    start = start_positions[i]\n",
    "    end = end_positions[i] + 1\n",
    "    string = tokenizer.decode(input_ids[start:end])\n",
    "    print(\"expected answer:\", answers[i][\"text\"][0])\n",
    "    print(\"preprocessing answer:\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f62f211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevents progress bar from flooding our document\n",
    "disable_progress_bar()\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    prepare_qa_train,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce7d69",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a82297",
   "metadata": {},
   "source": [
    "Upon preparing our dataset, fine-tuning a question answer model on top of pre-trained language model will be similar to other tasks, where we initialize a `AutoModelForQuestionAnswering` model, and follow the standard fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a76d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "fine_tuned_model_checkpoint = f\"{model_name}-fine_tuned-{task_name}\"\n",
    "\n",
    "if os.path.isdir(fine_tuned_model_checkpoint):\n",
    "    do_train = False\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(fine_tuned_model_checkpoint, cache_dir=cache_dir)\n",
    "else:\n",
    "    do_train = True\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23e188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=fine_tuned_model_checkpoint,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    # we set it to evaluate/save per epoch to avoid flowing console\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    save_total_limit=2,\n",
    "    do_train=do_train\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d64063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, overflow_to_sample_mapping. If offset_mapping, overflow_to_sample_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "/root/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 88524\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2768' max='2768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2768/2768 13:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.310800</td>\n",
       "      <td>1.138176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>1.133642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, overflow_to_sample_mapping. If offset_mapping, overflow_to_sample_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-fine_tuned-squad/checkpoint-1384\n",
      "Configuration saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-1384/config.json\n",
      "Model weights saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-1384/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-1384/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-1384/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, overflow_to_sample_mapping. If offset_mapping, overflow_to_sample_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-fine_tuned-squad/checkpoint-2768\n",
      "Configuration saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-2768/config.json\n",
      "Model weights saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-2768/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-2768/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-fine_tuned-squad/checkpoint-2768/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to distilbert-base-uncased-fine_tuned-squad\n",
      "Configuration saved in distilbert-base-uncased-fine_tuned-squad/config.json\n",
      "Model weights saved in distilbert-base-uncased-fine_tuned-squad/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-fine_tuned-squad/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-fine_tuned-squad/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "if trainer.args.do_train:\n",
    "    train_output = trainer.train()\n",
    "    # saving the model which allows us to leverage\n",
    "    # .from_pretrained(model_path)\n",
    "    trainer.save_model(fine_tuned_model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a7690",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd2a57",
   "metadata": {},
   "source": [
    "Evaluating our model also requires a bit more work on postprocessing front, hence we'll first use transformer's `pipeline` object for confirming the model we just trained is indeed learning by seeing if its predicted answer resembles ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c08321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file distilbert-base-uncased-fine_tuned-squad/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-fine_tuned-squad\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file distilbert-base-uncased-fine_tuned-squad/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-fine_tuned-squad\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file distilbert-base-uncased-fine_tuned-squad/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-uncased-fine_tuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
      "Didn't find file distilbert-base-uncased-fine_tuned-squad/added_tokens.json. We won't load it.\n",
      "loading file distilbert-base-uncased-fine_tuned-squad/vocab.txt\n",
      "loading file distilbert-base-uncased-fine_tuned-squad/tokenizer.json\n",
      "loading file None\n",
      "loading file distilbert-base-uncased-fine_tuned-squad/special_tokens_map.json\n",
      "loading file distilbert-base-uncased-fine_tuned-squad/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output answer matches expected answer:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.845287561416626,\n",
       " 'start': 177,\n",
       " 'end': 191,\n",
       " 'answer': 'Denver Broncos'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = datasets[\"validation\"][0]\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=fine_tuned_model_checkpoint,\n",
    "    tokenizer=fine_tuned_model_checkpoint\n",
    ")\n",
    "\n",
    "output = qa_pipeline({\n",
    "    \"question\": example[\"question\"],\n",
    "    \"context\": example[\"context\"]\n",
    "})\n",
    "answer_text = example[\"answers\"][\"text\"][0]\n",
    "print(\"output answer matches expected answer: \", output[\"answer\"] == answer_text) \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2041e0",
   "metadata": {},
   "source": [
    "For evaluation, we'll preprocess our dataset in a slightly different manner:\n",
    "\n",
    "- First, we technically don't need to generate labels.\n",
    "- Second, the \"fun\" part is to map our model's prediction back to original context's span. As a reminder, some of our features are overflowed inputs for the same given example. We'll be using example id for creating this mapping.\n",
    "- Last, but not least, we'll set the question part of our input sequence to `None`, this is for efficiently detecting if our predicted answer span is within the context portion of input sentence as opposed to the question portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38e43eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_qa_test(examples):\n",
    "    examples[\"question\"] = [question.strip() for question in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep overflows using a stride.\n",
    "    # This results in one example potentially generating several features when a context is\n",
    "    # long, each of those features having a context that overlaps a bit the previous\n",
    "    # feature's context to prevent chopping off answer span.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=doc_stride\n",
    "    )\n",
    "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # for offset mapping that are not part of context, set it to None so it's easy to determine\n",
    "        # if a token positiion is part of the context or not\n",
    "        offset_mapping = []\n",
    "        for k, offset in enumerate(tokenized_examples[\"offset_mapping\"][i]):\n",
    "            if sequence_ids[k] != 1:\n",
    "                offset = None\n",
    "\n",
    "            offset_mapping.append(offset)\n",
    "\n",
    "        tokenized_examples[\"offset_mapping\"][i] = offset_mapping\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4748abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_qa_test,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "validation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9c065",
   "metadata": {},
   "source": [
    "With our features, we can generate prediction which is a pair of start and end logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cbf618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id, overflow_to_sample_mapping. If offset_mapping, example_id, overflow_to_sample_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[  -8.055,   -9.14 ,   -9.33 , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -8.13 ,   -9.06 ,   -9.31 , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -8.4  ,   -8.65 ,   -9.53 , ..., -100.   , -100.   , -100.   ],\n",
       "        ...,\n",
       "        [  -5.02 ,   -8.89 ,   -9.48 , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -3.217,   -9.12 ,   -9.04 , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -4.484,   -9.04 ,   -9.46 , ..., -100.   , -100.   , -100.   ]],\n",
       "       dtype=float16),\n",
       " array([[  -8.19 ,   -9.32 ,   -9.195, ..., -100.   , -100.   , -100.   ],\n",
       "        [  -8.26 ,   -9.37 ,   -9.234, ..., -100.   , -100.   , -100.   ],\n",
       "        [  -8.125,   -9.37 ,   -8.805, ..., -100.   , -100.   , -100.   ],\n",
       "        ...,\n",
       "        [  -4.406,   -9.34 ,   -9.24 , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -2.547,   -9.22 ,   -9.   , ..., -100.   , -100.   , -100.   ],\n",
       "        [  -3.742,   -9.195,   -9.04 , ..., -100.   , -100.   , -100.   ]],\n",
       "       dtype=float16))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)\n",
    "raw_predictions.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d24c6",
   "metadata": {},
   "source": [
    "Having our original example, preprocessed features, generated predictions, we'll perform a final post-processing to generate predicted answer for each example. This process mainly involves:\n",
    "\n",
    "- Creating a map between examples and features.\n",
    "- Looping through all the examples, and for each example, loop through all its features to pick the best start and end logit combination from the n_best start and end logits.\n",
    "- During this picking out best answer span process, we'll automatically eliminate answers that are not inside the context, gives negative length (start position greater than end position), as well as answers that are too long (configurable with a max_answer_length parameter).\n",
    "- The \"proper\" way of computing score for each answer is to convert start and end logit into probability using a softmax operation, then taking a product of these two probabilities. Here, we'll skip the softmax and obtain the logit scores by summing start and end logits instead (log(ab)=log(a)+log(b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8819b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    raw_predictions,\n",
    "    n_best_size = 20,\n",
    "    max_answer_length = 30,\n",
    "    no_answer = False\n",
    "):\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    # build a dictionary that stores examples to features/chunks mapping\n",
    "    # key : example, value : list of features\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    cls_index = 0\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # for each example, loop through all its features/chunks for finding the best one\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        context = example[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            # model prediction for this feature\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            \n",
    "            # update minimum null prediction's score\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "            \n",
    "            # loop through all possibilities for `n_best_size` start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1:-n_best_size - 1:-1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1:-n_best_size - 1:-1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because indices\n",
    "                    # are out of bounds or correspond to input_ids that\n",
    "                    # are not part of the context section.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or len(offset_mapping[start_index]) < 2\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[end_index]) < 2\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"text\": context[start_char:end_char],\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = max(valid_answers, key=lambda x: x[\"score\"])\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction,\n",
    "            # we create a fake prediction to avoid failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "\n",
    "        example_id = example[\"id\"]\n",
    "        if no_answer:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example_id] = answer\n",
    "        else:\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5831203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10570/10570 [00:13<00:00, 768.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output answer matches expected answer:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(\n",
    "    datasets[\"validation\"],\n",
    "    validation_features,\n",
    "    raw_predictions.predictions\n",
    ")\n",
    "print(\"output answer matches expected answer: \", final_predictions[example[\"id\"]] == answer_text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28963f7",
   "metadata": {},
   "source": [
    "Squad primarily uses two metrics for model evaluation.\n",
    "\n",
    "- Exact Match: Measures percentage of predictions that perfectly matches any one of the ground truth answers.\n",
    "- Macro F1: Measures average overlap between prediction and ground truth answer.\n",
    "\n",
    "For context, screenshot below shows performance reported by the original Squad 2 paper [[5]](https://arxiv.org/abs/1806.03822). \n",
    "\n",
    "<img src=\"imgs/squad2_paper_results.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e6951b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 76.8306527909177, 'f1': 85.18462636575374}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_metric = evaluate.load(task_name, cache_dir=cache_dir)\n",
    "formatted_predictions = [\n",
    "    {\"id\": example_id, \"prediction_text\": answer}\n",
    "    for example_id, answer in final_predictions.items()\n",
    "]\n",
    "references = [{\"id\": example[\"id\"], \"answers\": example[\"answers\"]} for example in datasets[\"validation\"]]\n",
    "squad_metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c989200",
   "metadata": {},
   "source": [
    "That's a wrap for this document. We went through nitty gritty details on how to pre-process our inputs and post-process our outputs for fine-tuning a cross attention model on top of pre-trained language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52eb91",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1444b12",
   "metadata": {},
   "source": [
    "- [[1]](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) Notebook: Fine-tuning a model on a question-answering task\n",
    "- [[2]](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) Github: Huggingface Question Answering Examples\n",
    "- [[3]](https://huggingface.co/course/chapter7/7?fw=pt) Huggingface Course: Chapter 7 Main NLP tasks - Question answering\n",
    "- [[4]](https://www.pinecone.io/learn/reader-models/) Blog: Reader Models for Open Domain Question-Answering\n",
    "- [[5]](https://arxiv.org/abs/1806.03822) Paper: Pranav Rajpurkar, Robin Jia, et al. - Know What You Don't Know: Unanswerable Questions for SQuAD - 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
