{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d01b979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    html {\n",
       "        font-size: 18px !important;\n",
       "    }\n",
       "\n",
       "    body {\n",
       "        background-color: #FFF !important;\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    body .notebook-app {\n",
       "        background-color: #FFF !important;\n",
       "    }\n",
       "\n",
       "    #header {\n",
       "        box-shadow: none !important;\n",
       "    }\n",
       "\n",
       "    #notebook {\n",
       "        padding-top: 0px;\n",
       "    }\n",
       "\n",
       "    #notebook-container {\n",
       "        box-shadow: none;\n",
       "        -webkit-box-shadow: none;\n",
       "        padding: 10px;\n",
       "    }\n",
       "\n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border: 1px dashed #CCCCCC;\n",
       "    }\n",
       "\n",
       "    .edit_mode div.cell.selected {\n",
       "        border: 1px dashed #828282;\n",
       "    }\n",
       "\n",
       "    div.output_wrapper {\n",
       "        margin-top: 8px;\n",
       "    }\n",
       "\n",
       "    a {\n",
       "        color: #383838;\n",
       "    }\n",
       "\n",
       "    code,\n",
       "    kbd,\n",
       "    pre,\n",
       "    samp {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem !important;\n",
       "    }\n",
       "\n",
       "    h1 {\n",
       "        font-size: 2rem !important;\n",
       "        font-weight: 500 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: uppercase !important;\n",
       "    }\n",
       "\n",
       "    h2 {\n",
       "        font-size: 1.8rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: none !important;\n",
       "    }\n",
       "\n",
       "    h3 {\n",
       "        font-size: 1.5rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        font-style: italic !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    h4,\n",
       "    h5,\n",
       "    h6 {\n",
       "        font-size: 1rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    .prompt {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem;\n",
       "        text-align: right;\n",
       "        line-height: 1.21429rem;\n",
       "    }\n",
       "\n",
       "    /* INTRO PAGE */\n",
       "\n",
       "    .toolbar_info,\n",
       "    .list-container {\n",
       "        ;\n",
       "    }\n",
       "    /* NOTEBOOK */\n",
       "\n",
       "    div#header-container {\n",
       "        display: none !important;\n",
       "    }\n",
       "\n",
       "    div#notebook {\n",
       "        border-top: none;\n",
       "        font-size: 1rem;\n",
       "    }\n",
       "\n",
       "    div.input_prompt {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .code_cell div.input_prompt:after,\n",
       "    div.output_prompt:after {\n",
       "        content: '\\25b6';\n",
       "    }\n",
       "\n",
       "    div.output_prompt {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    div.input_area {\n",
       "        border-radius: 0px;\n",
       "        border: 1px solid #d8d8d8;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .rendered_html pre,\n",
       "    .rendered_html table,\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        border: 1px #828282 solid;\n",
       "        font-size: 0.75rem;\n",
       "        font-family: 'Menlo', monospace;\n",
       "    }\n",
       "\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        padding: 5px 10px;\n",
       "    }\n",
       "\n",
       "    .rendered_html th {\n",
       "        font-weight: normal;\n",
       "        background: #f8f8f8;\n",
       "    }\n",
       "\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "\n",
       "    div.output_html {\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    table.dataframe tr {\n",
       "        border: 1px #CCCCCC;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border-radius: 0px;\n",
       "    }\n",
       "\n",
       "    div.cell.edit_mode {\n",
       "        border-radius: 0px;\n",
       "        border: thin solid #CF5804;\n",
       "    }\n",
       "\n",
       "    span.ansiblue {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    span.ansigray {\n",
       "        color: #d8d8d8;\n",
       "    }\n",
       "\n",
       "    span.ansigreen {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    span.ansipurple {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    span.ansired {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    span.ansiyellow {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    div.output_stderr {\n",
       "        background-color: #D43132;\n",
       "    }\n",
       "\n",
       "    div.output_stderr pre {\n",
       "        color: #e8e8e8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython.CodeMirror {\n",
       "        background: #F8F8F8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #e8e8e8 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-gutters {\n",
       "        background: #F8F8F8;\n",
       "        border-right: 0px;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-linenumber {\n",
       "        color: #b8b8b8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-cursor {\n",
       "        border-left: 1px solid #585858 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-atom {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-property,\n",
       "    .cm-s-ipython span.cm-attribute {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable-2 {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-error {\n",
       "        background: #FFBDBD;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-tag {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-link {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-matchingbracket {\n",
       "        text-decoration: underline;\n",
       "         !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', '..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(css_style='custom2.css', plot_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bad779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2023-01-07\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "matplotlib  : 3.5.3\n",
      "pandas      : 1.4.3\n",
      "numpy       : 1.23.2\n",
      "torch       : 1.10.0a0+git36449ea\n",
      "evaluate    : 0.3.0\n",
      "transformers: 4.21.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass \n",
    "from time import perf_counter\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20bfe00",
   "metadata": {},
   "source": [
    "# Machine Translation with Huggingface Transformer mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aafeee",
   "metadata": {},
   "source": [
    "In this article we'll be leveraging [Huggingface's Transformer](https://github.com/huggingface/transformers) on our machine translation task as the library provides tons of pretrained models that we can either directly use or fine tune on our tasks.\n",
    "\n",
    "Machine translation is a sequence to sequence task, where we have a source and a target sentence as our dataset, and in transformer era, we use an encoder-decoder model architecture to solve for these type of problems. This type of architecture can also be used for tasks such as summarization, generative question answering, etc.\n",
    "\n",
    "We'll be using publicly available mT5 [[11]](https://arxiv.org/abs/2010.11934) pre-trained checkpoints, which is essentially a multi-lingual version of T5 (Text To Text Transfer Transformer) [[10]](https://arxiv.org/abs/1910.10683). Quick recap: T5 is a also a pre-trained language model based on un-labeled data, the main distinction is it re-formulates all text based NLP problem, be it classification (e.g. GLUE or SuperGLUE benchmarks), transalation, summarization, question and answering, into a sequence to sequence setting that can be solved by an encoder-decoder architecture as shown in its original diagram.\n",
    "\n",
    "<img src=\"imgs/t5.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "Some key takeaways:\n",
    "\n",
    "- Notice from the diagram above, apart from pre-training, it also introduces this notion of adding task specific prefix to specify which task the model should perform during fine-tuning stage.\n",
    "- For pre-training, T5 uses a masked language modeling \"span-corruption\" objective, where consecutive spans of input tokens are replaced with a special sentinel token and the model is trained to reconstruct these masked-out tokens as shown below. The original paper also explored various strategies and shared that:\n",
    "    - Denoising objectives outperforms language modeling and deshuffling for pre-training.\n",
    "    - Performance-wise different denoising variants did not lead to remarkable differences, though can lead to different sequence length and therefore training speed.\n",
    "\n",
    "<img src=\"imgs/t5_span_masking.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "- The primary dataset used for T5 is C4 (Colossal Clean Crawled Corpus), a post-processed corpus from common crawl, after experimenting with different dataset variants, the main lession is a not surprising finding that pre-training on in-domain unlabeled data can improve performance on in-domain downstream tasks, and given that obtaining pre-training corpus should ideally be cheap compared to human annotation, they suggested using larger pre-training datasets whenever possible.\n",
    "- Scaling up model size continues to be a promising approach for achieving better performance. i.e. Training a smaller model on more data was often outperformed by training a larger model for fewer steps.\n",
    "\n",
    "Apart from inheriting many of the properties from T5, some additional key results from mT5:\n",
    "\n",
    "- When pre-training on multilingual dataset (in the case of mT5, 101 languages), one important decision is how to sample data from each language. One common sampling strategy is: $|L|^{\\alpha}$, where $|L|$ denotes number of examples in a given lanuage, and $\\alpha$ is a hyperparameter that we can tune that controls how much to boost the probability of training on lower resource languages. The original paper reports a 0.3 value which gives a reasonable compromise between performance on high and low resource languages.\n",
    "- Scaling up models is an effective strategy for cross lingual representation learning, the original paper reports five different model sizes: Small (300M parameters), Base (580M), Large (1.2B), XL (3.7B), and XXL (13B). As the model grows larger, not only does it offer better performance on downstream multi-lingual benchmark suite, but also narrows the performance gaps with similar sized mono-lingual pre-trained models on mono-lingual benchmarks. Note that scaling up also includes vocabulary size. mT5 uses a sentencepiece tokenizer and scaled up to 250,000 wordpieces as vocabulary size compared to the 32,000 of T5.\n",
    "- Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model doesn't include a task prefix like the original T5 model and has to be fine-tuned before it is usable on a downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8593b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    cache_dir: str = \"./translation\"\n",
    "    data_dir: str = os.path.join(cache_dir, \"wmt16\")\n",
    "    source_lang: str = \"de\"\n",
    "    target_lang: str = \"en\"    \n",
    "    \n",
    "    batch_size: int = 16\n",
    "    num_workers: int = 4\n",
    "    seed: int = 42\n",
    "    max_source_length: int = 128\n",
    "    max_target_length: int = 128\n",
    "\n",
    "    lr: float = 0.0005\n",
    "    weight_decay: float = 0.01\n",
    "    epochs: int = 20\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_checkpoint: str = \"google/mt5-small\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b28b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e0f7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029b269",
   "metadata": {},
   "source": [
    "We'll be using Multi30k dataset [[9]](https://arxiv.org/abs/1605.00459), used in WMT16 (Multimodal Machine Translation 2016) conference, to demonstrate transfomer model in a machine translation task. This is a moderate sized German to English translation dataset, whose size is around 29K. That way, we can get our results without waiting too long. We'll start off by downloading the raw dataset and extracting them. Feel free to swap this step with any other machine translation dataset.\n",
    "\n",
    "Utility scripts for creating this data reside [here](https://github.com/ethen8181/machine-learning/blob/master/deep_learning/seq2seq/translation_mt5/translation_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c237a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1207136/1207136 [00:00<00:00, 5663556.99B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46329/46329 [00:00<00:00, 1427989.17B/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43905/43905 [00:00<00:00, 1356274.75B/s]\n"
     ]
    }
   ],
   "source": [
    "from translation_utils import download_file, create_translation_data\n",
    "\n",
    "# files are downloaded from\n",
    "# http://www.statmt.org/wmt16/multimodal-task.html\n",
    "urls = [\n",
    "    'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz',\n",
    "    'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz',\n",
    "    'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    download_file(url, config.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1979c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmt16_task1_test.tar.gz  test.en   train.en\t    val.de  validation.tar.gz\n",
      "test.de\t\t\t train.de  training.tar.gz  val.en\n"
     ]
    }
   ],
   "source": [
    "!ls $config.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9e381",
   "metadata": {},
   "source": [
    "The original dataset splits source and target language into two separate files, e.g. train.de, train.en is our training dataset for German and English. This type of format is useful when we wish to train a tokenizer on top of the source or target language independently. On the other hand, having source and target pair together in one single file makes it easier to load them in batches for training or evaluating our machine translation model. We'll create a paired dataset, and [load it into a dataset](https://huggingface.co/docs/datasets/loading#csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49617cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['./train.tsv'], 'val': ['./val.tsv'], 'test': ['./test.tsv']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {}\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    source_input_path = os.path.join(config.data_dir, f\"{split}.{config.source_lang}\")\n",
    "    target_input_path = os.path.join(config.data_dir, f\"{split}.{config.target_lang}\")\n",
    "    output_path = os.path.join(config.cache_dir, f\"{split}.tsv\")\n",
    "    create_translation_data(source_input_path, target_input_path, output_path)\n",
    "    data_files[split] = [output_path]\n",
    "\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ddc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-685768bd6cf26331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-685768bd6cf26331/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 2885.99it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 65.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-685768bd6cf26331/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 686.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['de', 'en'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['de', 'en'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['de', 'en'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\n",
    "    \"csv\",\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[config.source_lang, config.target_lang],\n",
    "    data_files=data_files\n",
    ")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b3188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en': 'Two young, White males are outside near many bushes.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can acsess the split, and each record/pair with the following syntax\n",
    "sample = dataset_dict[\"train\"][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb041f8b",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a9c0c",
   "metadata": {},
   "source": [
    "Before we start building our model, we'll first go over how to quantitatively evaluate one. Evaluating a generative model's machine translation output is not as black and white as say an output for classification task, as given a source sentence, there might be multiple equally good target sentence. Popular automated metrics belong to the ROUGE and BLEU family, which both measures correspondence between a machine generated output to that of a human via word/token overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7ebe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\", cache_dir=config.cache_dir)\n",
    "bleu_score = evaluate.load(\"bleu\", cache_dir=config.cache_dir)\n",
    "sacrebleu_score = evaluate.load(\"sacrebleu\", cache_dir=config.cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9e3c7",
   "metadata": {},
   "source": [
    "**BLEU (Bilingual Evaluation Understudy)** [[6]](https://aclanthology.org/P02-1040/) works first by comparing n-grams of the machine generated translation with n-grams of human provided reference translation, and counting the number of matches between the two, the more matches, the merrier. Its first component boils down to a modified precision metric:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Modified Precision}=\\frac{\\text{capped number of overlapping words}}{\\text{total number of words in generated summary}}\n",
    "\\end{align}\n",
    "\n",
    "Where in the numerator, we will give each word credit only up to the maximum number of times that word appeared in its reference sentence.\n",
    "\n",
    "e.g. Given the following example reference and candidate\n",
    "\n",
    "```\n",
    "Candidate: the the the cat mat\n",
    "Reference: the cat is on the mat\n",
    "```\n",
    "\n",
    "We would have a precision of 3 / 5. As there are 5 total candidate words, making it our the denominator. As for numerator, \"the\", appeared 3 times in the candidate sentence but is then capped to 2 as, \"the\", only appeared 2 times in its reference sentence, and \"cat\" and \"mat\" both appeared 1 time.\n",
    "\n",
    "\n",
    "BLEU introduces two additional adjustments on top of its modified precision cornerstone.\n",
    "\n",
    "- It combines n-gram modified precision up till 4-gram and then computes the geometric mean.\n",
    "- It introduces a brevity penalty to compensate for its lack of recall component, making it an adjustment that penalize translations that are too short.\n",
    "\n",
    "e.g. If we have a candidate, reference pair like below:\n",
    "\n",
    "```\n",
    "Candidate: of the\n",
    "Reference: It is the practical guide for the army always to heed the directions of the party.\n",
    "```\n",
    "\n",
    "We would obtain a modified unigram precision of 2/2. Intuitively, this candidate translation's precision metric is a bit inflated due to its short nature. To account for this, sentence brevity penalty is introduced as the second component:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{BP} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } c > r \\\\\n",
    "\\exp \\big(1-\\frac{r}{c}\\big) & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "where $c$ is the word length for the candidate sentences, and $r$ is the best match length for each candidate sentence in the corpus. e.g. given 2 references with lengths 12, 15 words and the candidate translation is 12 words, our brevity penalty will be 1 as the candidate's length is the same as any reference translation, and the closest reference's sentence length is termed \"best match length\". Note, this brevity penalty is computed by summing over the entire corpus to allow some freedom at sentence level.\n",
    "\n",
    "Finally, putting it all together, BLEU score is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{BLEU} = \\text{BP} \\cdot \\exp \\bigg( \\sum_{n=1}^{N} \\frac{1}{N} \\log p_n \\bigg)\n",
    "\\end{align}\n",
    "\n",
    "Where $p_n$ is the modified precision for $n$gram, and the second term on the right represents geometric mean for these $n$gram modified precision, where $N$ is typically set to 4 for 4-grams. The original paper is a great read for more in depth explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13260629",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7143b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.6434588841607617,\n",
       " 'precisions': [0.8571428571428571, 0.6666666666666666, 0.6, 0.5],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1666666666666667,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we can see the metric returns necessary components including\n",
    "# 'bleu': bleu score\n",
    "# 'precisions': geometric mean of n-gram precisions\n",
    "# 'brevity_penalty': brevity penalty\n",
    "# we can confirm precision 1 is indeed 6 / 7\n",
    "bleu_score.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3659314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 64.34588841607616,\n",
       " 'counts': [6, 4, 3, 2],\n",
       " 'totals': [7, 6, 5, 4],\n",
       " 'precisions': [85.71428571428571, 66.66666666666667, 60.0, 50.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 7,\n",
       " 'ref_len': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result from another bleu implementation\n",
    "sacrebleu_score.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14777aa",
   "metadata": {},
   "source": [
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** [[8]](https://aclanthology.org/W04-1013/) is also based on calculating overlap tokens between our system's generated summary versus reference summary (ground truth typically written by humans). In the original introduction of ROUGE, it was more focused on recall side of the picture compared to Bleu, which was more precision oriented. Nowadays, we'll commonly see it based on computing f1 score for the overlap. Where:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Recall}=\\frac{\\text{number of overlapping words}}{\\text{total number of words in reference summary}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Precision}=\\frac{\\text{number of overlapping words}}{\\text{total number of words in generated summary}}\n",
    "\\end{align}\n",
    "\n",
    " There are different variants of rouge score, the most popular ones being:\n",
    "\n",
    "- `rouge-{n}`: computes the rouge score for matching n-grams. Most common ones includes rouge-1 for unigram overlap, and rouge-2 for bigram overlap.\n",
    "- `rougeL`: L here stands for [longest common subsequence](https://en.wikipedia.org/wiki/Longest_common_subsequence), i.e. the longest sequence of words that are not necessarily consecutive, but still in order that are shared between both. Without dependency on consecutive n-grams, this variant aim to capture sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e97abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge 1:\n",
    "# precision = 6 / 7\n",
    "# recall = 6 / 6\n",
    "# 2 * (precision * recall) / (recall + precision)\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary],\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba2fca",
   "metadata": {},
   "source": [
    "Important things to keep in mind:\n",
    "\n",
    "- Given the text matching nature of these metrics, we can imagine they can't really compare different wordings that have same semantic meaning or ensure sentence level grammatical correctness. e.g. dropping a potentially less important function word like \"a\" will incur the same penalty as missing a word that's crucial. Hence further human judgement might be needed to ensure our model is indeed generating coherent results.\n",
    "- Prior to computing these metrics, both reference and candidate sentences need to be normalization and tokenized, make sure our pipelines are using the same pre-processing steps to ensure we are conducting an apples to apples comparison as different pre-processing variations can have signficant affect on the final numbers. This issue is raised in SACREBLEU [[7]](https://aclanthology.org/W18-6319/), which aims to provide a tool to faciliate standard configuration on BLEU metrics.\n",
    "- These scores ranges from 0.0 to 1.0, where 1.0 indicates perfect overlap with reference sentences. Given the free form nature of these type of tasks, adding more reference sentences for a given candidate sentence can improve these scores. Note that even human do not achieve a perfect score of 1.0.\n",
    "- Despite having their own drawbacks, they are widely use given their favorable properties including, language independent, ease of computation (compared to sending results for human judgements), and have been shown to correlate reasonably well with human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c70971",
   "metadata": {},
   "source": [
    "## Fine Tuning Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a587bc",
   "metadata": {},
   "source": [
    "Same as usual, we'll load our tokenizer and tokenize our raw text, which includes our source and target sentence.\n",
    "\n",
    "Feel free to try out different model checkpoints. We'll pick a small checkpoint for rapid experimentation purpose as well as a multi-lingual one such as mT5 to cope with our multi-lingual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3493393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 300176768\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint, cache_dir=config.cache_dir)\n",
    "\n",
    "model_name = config.model_checkpoint.split(\"/\")[-1]\n",
    "fine_tuned_model_checkpoint = os.path.join(\n",
    "    config.cache_dir,\n",
    "    f\"{model_name}_{config.source_lang}-{config.target_lang}\"\n",
    ")\n",
    "if os.path.isdir(fine_tuned_model_checkpoint):\n",
    "    do_train = False\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(fine_tuned_model_checkpoint, cache_dir=config.cache_dir)\n",
    "else:\n",
    "    do_train = True\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_checkpoint, cache_dir=config.cache_dir)\n",
    "\n",
    "print(\"number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e59b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_fn(examples):\n",
    "    \"\"\"\n",
    "    Generate the input_ids and labels field for huggingface dataset/dataset dict.\n",
    "\n",
    "    Truncation is enabled where we cap the sentence to the max length. Padding will be done later\n",
    "    in a data collator, so we pad examples to the longest length within a mini-batch and not\n",
    "    the whole dataset.\n",
    "    \"\"\"\n",
    "    sources = examples[config.source_lang]\n",
    "    targets = examples[config.target_lang]\n",
    "    model_inputs = tokenizer(sources, max_length=config.max_source_length, truncation=True)\n",
    "\n",
    "    # setup the tokenizer for targets,\n",
    "    # huggingface expects the target tokenized ids to be stored in the labels field\n",
    "    # note, newer version of tokenizer supports a text_target argument, where we can create\n",
    "    # source and target sentences in one go\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=config.max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dace893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function batch_tokenize_fn at 0x7f684204e0d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:01<00:00, 16.72ba/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 40.31ba/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.12ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_tokenized = dataset_dict.map(\n",
    "    batch_tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names\n",
    ")\n",
    "dataset_dict_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b36d45",
   "metadata": {},
   "source": [
    "Data collator used for seq2seq model, `DataCollatorForSeq2Seq`, needs to pad both input and labels. One thing to note is input ids need to padded with tokenizer's padding token, whereas labels need to padded with a sentinel value, `-100`, which is the default value PyTorch uses for ignoring these indices during loss computation.\n",
    "\n",
    "Apart from that, there's also another fieldspecific to encoder-decoder models called `decoder_input_ids`. This field contains the input ids that will be fed to the decoder, and most of the time, they are shifted versions of labels with special tokens at the beginning. This is required for two main reasons: 1. to ensure decoder only sees the previous ground truth labels during training and not the current or future ones. 2. Introduce teacher forcing, where during training, our decoder always gets the ground-truth token in the next step, no matter what model's prediction are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca86113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 49165,    259,  59120,  24014,    265,  55011,   1926,    859,  20757,\n",
       "            278,    281,    442,  38675,    265,   3393,    295,  50339,   5923,\n",
       "            260,      1],\n",
       "        [ 20327,    967,  55011,    749,  52968,  77862,    278, 125312,    278,\n",
       "            888, 224236,   9600,  11581,    260,      1,      0,      0,      0,\n",
       "              0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), 'labels': tensor([[ 12796,  14802,    261,   4665,  38500,    418,  23109,   9137,   3506,\n",
       "         140821,    299,    260,      1,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100],\n",
       "        [   746,  14467,    692,    281,   6528,   1994,    263,    418,    259,\n",
       "          45814,    259,    262,    259, 102206,   8001,   3175,   2974,    260,\n",
       "              1]]), 'decoder_input_ids': tensor([[     0,  12796,  14802,    261,   4665,  38500,    418,  23109,   9137,\n",
       "           3506, 140821,    299,    260,      1,      0,      0,      0,      0,\n",
       "              0],\n",
       "        [     0,    746,  14467,    692,    281,   6528,   1994,    263,    418,\n",
       "            259,  45814,    259,    262,    259, 102206,   8001,   3175,   2974,\n",
       "            260]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "features = [dataset_dict_tokenized[\"train\"][i] for i in range(2)]\n",
    "output = data_collator(features)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801cb99f",
   "metadata": {},
   "source": [
    "For seq2seq models, we'll use seq2seq variants of TrainingArguments and Trainer. One of the most important difference is setting `predict_with_generate=True`. Decoder performs inference by predicting tokens one by one, and this is implemented by the model's `generate` method. Setting predict_with_generate=True tells Seq2SeqTrainer to use that method for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d46d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config.model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = os.path.join(config.cache_dir, f\"{model_name}_{config.source_lang}-{config.target_lang}\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    weight_decay=config.weight_decay,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=config.epochs,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    do_train=do_train,\n",
    "    # careful when attempting to train t5 models on fp16 mixed precision,\n",
    "    # the model was trained on bfloat16 mixed precision, and mixing different mixed precision\n",
    "    # type might result in nan loss\n",
    "    # https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315\n",
    "    fp16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0d1478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute rouge and bleu metrics for seq2seq model generated prediction.\n",
    "    \n",
    "    tip: we can run trainer.predict on our eval/test dataset to see what a sample\n",
    "    eval_pred object would look like when implementing custom compute metrics function\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries, which is in ids into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode labels, a.k.a. reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    )\n",
    "    score = sacrebleu_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    result[\"sacrebleu\"] = score[\"score\"]\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da5c7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset_dict_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_dict_tokenized[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06882de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should take around 4117.78 seconds on a single V100 GPU\n",
    "if trainer.args.do_train:\n",
    "    t1_start = perf_counter()\n",
    "    train_output = trainer.train()\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Training elapsed time:\", t1_stop - t1_start)\n",
    "\n",
    "    # saving the model which allows us to leverage\n",
    "    # .from_pretrained(model_path)\n",
    "    trainer.save_model(fine_tuned_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b15b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1014\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.863990843296051,\n",
       " 'eval_rouge1': 0.7403,\n",
       " 'eval_rouge2': 0.5408,\n",
       " 'eval_rougeL': 0.7178,\n",
       " 'eval_sacrebleu': 42.9889,\n",
       " 'eval_runtime': 23.4856,\n",
       " 'eval_samples_per_second': 43.175,\n",
       " 'eval_steps_per_second': 2.725}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad836867",
   "metadata": {},
   "source": [
    "These numbers we got by fine-tuning a pre-trained MT5 model are pretty solid when comparing with, [MarianMT](https://huggingface.co/transformers/model_doc/marian.html), an already pretrained machine translation model, `Helsinki-NLP/opus-mt-de-en`. Note, MarianMT model only has 74,410,496 parameters, which is a lot smaller compared to mt5-small's 300,176,768 parameters.\n",
    "\n",
    "```\n",
    "{'eval_loss': 1.1351912021636963, 'eval_rouge1': 0.7198, 'eval_rouge2': 0.4904, 'eval_rougeL': 0.6935, 'eval_sacrebleu': 40.2274, 'eval_runtime': 40.1401, 'eval_samples_per_second': 25.262, 'eval_steps_per_second': 1.594}\n",
    "```\n",
    "\n",
    "These numbers are computed using [this script](https://github.com/ethen8181/machine-learning/blob/master/deep_learning/seq2seq/translation_mt5/seq2seq_eval.py).\n",
    "\n",
    "Apart from quantitative metric evaluation, we can also look at sample generated translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "802ef0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation(model, tokenizer, example):\n",
    "    \"\"\"print out the source, target and predicted raw text.\"\"\"\n",
    "    source = example[config.source_lang]\n",
    "    target = example[config.target_lang]\n",
    "    input_ids = tokenizer(source)[\"input_ids\"]\n",
    "    input_ids = torch.LongTensor(input_ids).view(1, -1).to(model.device)\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=20)\n",
    "    prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print('source: ', source)\n",
    "    print('target: ', target)\n",
    "    print('prediction: ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd8d6250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "target:  A group of men are loading cotton onto a truck\n",
      "prediction:  A group of men are loading cotton on a truck.\n"
     ]
    }
   ],
   "source": [
    "example = dataset_dict['val'][0]\n",
    "generate_translation(model, tokenizer, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c7677",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349316d3",
   "metadata": {},
   "source": [
    "- [[1]](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb) Colab: Fine-tuning a model on a translation task\n",
    "- [[2]](https://huggingface.co/course/chapter7/4?fw=pt) Hugginface Course: Translation\n",
    "- [[3]](https://huggingface.co/course/chapter7/5?fw=pt) Hugginface Course: Summarization\n",
    "- [[4]](https://huggingface.co/docs/transformers/model_doc/t5) Huggingface Model Doc: T5\n",
    "- [[5]](https://cloud.google.com/translate/automl/docs/evaluate)  Google Cloud AutomML Translation Doc: Evaluating Models \n",
    "- [[6]](https://aclanthology.org/P02-1040/) Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu - Bleu: a Method for Automatic Evaluation of Machine Translation - 2002\n",
    "- [[7]](https://aclanthology.org/W18-6319/) Matt Post - A Call for Clarity in Reporting BLEU Scores - 2018\n",
    "- [[8]](https://aclanthology.org/W04-1013/) Chin-Yew Lin - ROUGE: A Package for Automatic Evaluation of Summaries - 2004\n",
    "- [[9]](https://arxiv.org/abs/1605.00459) Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia - Multi30K: Multilingual English-German Image Descriptions - 2016\n",
    "- [[10]](https://arxiv.org/abs/1910.10683) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, et al. - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 2019\n",
    "- [[11]](https://arxiv.org/abs/2010.11934) Linting Xue, Noah Constant, Adam Roberts, et al. - mT5: A massively multilingual pre-trained text-to-text transformer - 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
